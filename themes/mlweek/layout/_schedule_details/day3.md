## Deep learning
In the last decade, neural networks have achieved some remarkable successes, from image search to voice recognition (Siri, Google Now, etc.).  The advances that have lead to this sea change mostly fall under the appellation “deep learning”.  We’ll discuss what that means and build some simple examples.
![Artificial_neural_network](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Artificial_neural_network.svg/560px-Artificial_neural_network.svg.png)
Source: WikiMedia Commons

## Dimensionality reduction
Machine learning walks hand in hand with Big Data.  Data can be big not only because we have a lot of it, but because it is wide, that is, embedded in very highly dimensional spaces.  Dimensionality reduction gives us tools for finding the components of our data that matter most and -- good for us -- help us to run our programs faster.
![GaussianScatterPCA](https://upload.wikimedia.org/wikipedia/commons/thumb/1/15/GaussianScatterPCA.png/512px-GaussianScatterPCA.png)

## Anomaly Detection
Algorithms can be classified as online or offline, according to whether we can look at all the data (offline) or merely examine a stream of data as it passes (online).  A common concern in online data is identifying outliers, sometimes when we don’t know very much about what the outliers will look like.  For example, we might want to watch our server logs for unusual events so that we can explore further.

## Large Data
When data sets become quite large, we sometimes have to adopt special techniques to deal with the size.  This can be quite time consuming (because the data is so big!), but we’ll nonetheless talk about some approaches so that you aren’t stymied when your data grows large.
